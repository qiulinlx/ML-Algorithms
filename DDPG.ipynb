{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afab6b6c-1714-45ec-9df5-1b406e7f384e",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient\n",
    "\n",
    "DRL algorithm for continuous control.  DDPG is adapted specifically for environments with continuous action spaces. \n",
    "It extends DQN to work with the continuous action space by introducing a deterministic actor that directly outputs continuous actions. The algorithm is model-free, online and off-policy.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de38a41-5215-4405-8443-046f10e602fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experience Replay buffer\n",
    "Typically applied to off-policy reinforcement learning algorithms. The set $\\mathcal D$ shows all previous experiences of the agent. The idea is to capture all the samples generated by an agent interacting with its environment and then storing them for later reuse. This does not store associated values (Q-values) but rather the raw data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd44370-1b71-42f0-9764-8290a5406650",
   "metadata": {},
   "source": [
    "### Continuous action spaces.\n",
    "When the action space is continuous, we canâ€™t exhaustively evaluate the space, and solving the optimization problem is highly non-trivial. Normal optimization algorithm would make calculating $max_a Q^*(s,a)$ a painfully expensive subroutine that would need to be run every time the agent must decide on an action. \n",
    "Since action space is continuous, the function $Q^*(s,a)$ is presumed to be differentiable with respect to the action argument. We can exploit the differentiability of a the system by using gradient descent methods to approximate a policy $\\mu(a)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fafcc89-7ce6-4953-ad23-c5d382be290e",
   "metadata": {},
   "source": [
    "## The Q-learning side\n",
    "the Bellman equation describing the optimal action-value function, $Q^*(s,a)$. The optimal action can be found by solving\n",
    "$a^*(s)= argmax Q^*(s,a)$\n",
    "\n",
    "If we use a neural network ($Q_{\\phi}$) to approximate the optimal action-value function. DDPG employs the use of mean-squared Bellman error (MSBE) function which estimates how close $Q_{\\phi}$ comes close to satisfying the Bellman equation \n",
    "\n",
    "$L(\\phi, D)= E[(Q_{\\phi}(s,a)-(r+\\gamma(1-d)Q_{\\phi}(s', a')))]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce623de-ac22-4a30-993a-33692afa984f",
   "metadata": {},
   "source": [
    "#### Target Networks\n",
    "Use of a Target network to deal with non-stationary target values and make the learning more stable. The target\n",
    "is the value we want our approximator $Q_{\\phi}$ to take on.\n",
    "\n",
    "$r+\\gamma(1-d)Q_{\\phi}(s', a')$\n",
    "The problem is that the targer takes on the parameters $\\phi$ that we are trying to train. This makes MSBE minimization unstable. The solution is to use a set of parameters which comes close to $\\phi$, but with a time delay. This set of parameters will be trained using a **Target network**. The parameters of the target network are denoted $\\phi_{target}$\n",
    "Target network is updated once per main network by polyak averaging:\n",
    "\n",
    "$\\phi_{target} \\leftarrow \\phi_{target}p +(1-p)\\phi_{target}$\n",
    "\n",
    "Polyak Averaging is an optimization technique that sets final parameters to an average of (recent) parameters visited in the optimization trajectory\n",
    "\n",
    "**The Target policy network**  computes an action which approximately maximizes $Q_{\\phi_{\\text{targ}}}$. The target policy network is found the same way as the target Q-function: by polyak averaging.\n",
    "\n",
    "Q-learning in DDPG is performed by minimizing the following MSBE loss with stochastic gradient descent:\n",
    "\n",
    "$L(\\phi, D)= E[(Q_{\\phi}(s,a)-(r+\\gamma(1-d)Q_{\\phi}(s', \\mu_{\\theta_{target}}(s)')))]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d6310-a854-42b0-b3f9-49aaf497fb67",
   "metadata": {},
   "source": [
    "## The Policy Learning Side\n",
    "The goal is to learn a deterministic policy $\\mu_{\\theta}(s)$ which gives the action that maximizes $Q_{\\phi}(s,a)$. We can perform gradient ascent (with respect to policy parameters only) to solve\n",
    "\n",
    "$\\max_{\\theta} \\underset{s \\sim {\\mathcal D}}{{\\mathrm E}}\\left[ Q_{\\phi}(s, \\mu_{\\theta}(s)) \\right].$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb96273-b071-449d-88c2-19469e2b028a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
