{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1338fba9-7370-4de3-8e4d-893aca894b77",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2fb46-6762-4070-a2c0-755230a47ad9",
   "metadata": {},
   "source": [
    "Reinforcement learning policy that will find the next best action, given a current state. It chooses this action at random and aims to maximize the reward. \\\n",
    "Q-learning is a model-free, off-policy algorithm that chooses the best action based on the current. The agent uses predictions of the environmentâ€™s expected response to move forward. It does not learn via the reward system, but rather, trial and error. This is a type of **Temporal Difference** Learning (Unsupervised technique in which the learning agent learns to predict the expected value of a variable occurring at the end of a sequence of states.)\\\n",
    "Let us begin implementing the algorithm in the pre-made OpenAI gym environment, FrozenLake-v1. We start by importing the necessary packages and rendering the environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71032791-c8f9-471f-bb25-ff676df87c40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c0dcc6-e076-46a7-a1eb-5146182f2909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "environment = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode='human')\n",
    "environment.reset()\n",
    "environment.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f6a68-9816-4b36-bc72-c9866faea2ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Q-Table** \n",
    "A Q-table gives the value of each action given the state that the agent is in. Each cell corresponds to one state-action pair value. \n",
    "|State|Actions||||\n",
    "|-----|-----|-----|-----|-----|\n",
    "|     |  L  |  R  |  U  |  D  |\n",
    "|  A  | 0.1 | 0.3 | 0   | 0.4 |\n",
    "|  B  | 0.2 | 0.7 | 0.5 | 0   |\n",
    "|  C  | 0.6 | 0   | 0.7 | 0.9 |\n",
    "|  D  | 0   | 0.2 | 0.1 | 0   |\n",
    "\n",
    "The Q-table is always initialised as a zero matrix before the agent begins exploring. The table will always have dimensions (states x actions), in our case it is 16 x 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d770817a-8f69-4bbf-a964-c1b645842fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Our table has the following dimensions:\n",
    "# (rows x columns) = (states x actions) = (16 x 4)\n",
    "qtable = np.zeros((16, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "022180c4-afac-4d50-8c1e-e6854dcb3c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, the gym library can also directly g\n",
    "# give us the number of states and actions using \n",
    "# \"env.observation_space.n\" and \"env.action_space.n\"\n",
    "nb_states = environment.observation_space.n  # = 16\n",
    "nb_actions = environment.action_space.n      # = 4\n",
    "qtable = np.zeros((nb_states, nb_actions))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56113869-7ea3-4867-b6f6-b26f3fb5a0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams.update({'font.size': 17})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf5c4581-907f-441f-90ca-b149054981c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table before training:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#We re-initialize the Q-table\n",
    "qtable = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
    "\n",
    "# Hyperparameters\n",
    "episodes = 200       # Total number of episodes\n",
    "alpha = 0.5            # Learning rate\n",
    "gamma = 0.9            # Discount factor\n",
    "\n",
    "# List of outcomes to plot\n",
    "outcomes = []\n",
    "\n",
    "print('Q-table before training:')\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b1a78-f19b-42ff-b683-c1323576b9d0",
   "metadata": {},
   "source": [
    "We now begin the training portion of the algorithm. This is where the agent explores the environment and gathers data on the q-values of the various states and actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e774d-b22b-4989-9f86-b9111bfb0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for _ in range(episodes):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    # By default, we consider our outcome to be a failure\n",
    "    outcomes.append(\"Failure\")\n",
    "\n",
    "    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        state=np.asarray(state)\n",
    "        #print(\"Currently\", state)\n",
    "        # Choose the action with the highest value in the current state\n",
    "        if np.max(qtable[state[0]]) > 0:\n",
    "          action = np.argmax(qtable[state[0]])\n",
    "          \n",
    "\n",
    "        # If there's no best action (only zeros), take a random one\n",
    "        else:\n",
    "          action = environment.action_space.sample()\n",
    "             \n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, done, truncated, info = environment.step(action)\n",
    "\n",
    "        # Update Q(s,a)\n",
    "        \n",
    "        qtable[state[0], action] = qtable[state[0], action] + alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state[0], action])\n",
    "      \n",
    "       \n",
    "        state[0]=new_state\n",
    "\n",
    "        #print('this is the new state', state)\n",
    "\n",
    "        # If we have a reward, it means that our outcome is a success\n",
    "        if reward:\n",
    "          outcomes[-1] = \"Success\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e45f5-ec92-4caf-b0e6-60356cf3340f",
   "metadata": {},
   "source": [
    "## **The Bellman Equation**\n",
    "The equation is used to determine the value of a particular state and deduce how good it is to be in that state.The Q-function $Q(s,a)$ gives the value of the current state, $R(s,a)$ is the reward, $Max Q(s',a')$ is the maximum expected future reward. \n",
    "\n",
    "$New  Q(s,a)= Q(s,a) + \\alpha [R(s,a) +\\gamma Max Q(s',a') -Q(s,a)]$\n",
    "\n",
    "The Bellman Equation is used to explore and update the Q-table. The agent is improved by performing a greedy search where only the maximum reward received for the particular set of actions in that particular state is considered. Q-learning considers previous states when considering the next action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36e095-175f-4a42-bdc2-4a3a23c8a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Q-table after training:')\n",
    "print(qtable)\n",
    "\n",
    "# Plot outcomes\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.xlabel(\"Run number\")\n",
    "plt.ylabel(\"Outcome\")\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('#efeeea')\n",
    "plt.bar(range(len(outcomes)), outcomes, color=\"#0A047A\", width=1.0)\n",
    "plt.show()\n",
    "\n",
    "nb_success = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7cd5ed-a265-410b-b9ce-ee064627060c",
   "metadata": {},
   "source": [
    "The Q-table after training should contain non-zero elements corresponding to the value of each state. The plot provided shows the amount of successes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b4a0b-c0d2-41a5-83d2-960bc04463ff",
   "metadata": {},
   "source": [
    "The Algorithm can now be evaluated or performs after training. In this case, it is whether the agent can reach the reward without running into any terminal states (holes). Q-learning works by selecting the action with the highest Q-value for each given state each time. If there is no highest value it will simply choose a random action (and thus state) and go from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101ef9d-96ab-4235-b558-91ea20f32963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "for _ in range(100):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Until the agent gets stuck or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Choose the action with the highest value in the current state\n",
    "        if np.max(qtable[state[0]]) > 0:\n",
    "          action = np.argmax(qtable[state[0]])\n",
    "\n",
    "        # If there's no best action (only zeros), take a random one\n",
    "        else:\n",
    "          action = environment.action_space.sample()\n",
    "             \n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, done, truncated, info = environment.step(action)\n",
    "\n",
    "        # Update our current state\n",
    "\n",
    "        state=[new_state]\n",
    "\n",
    "        # When we get a reward, it means we solved the game\n",
    "        nb_success += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40cdcc7-ef45-47cd-832d-affac02a7c60",
   "metadata": {},
   "source": [
    "We now begin checking the success rate by running the algorithm several times to attain how succesful the method is. Note that the agent is not obeying any set policy but rather acts according to values (**Value-Based methods**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8e1b7-b076-460e-a58e-5b554067f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check our success rate!\n",
    "print (f\"Success rate = {nb_success/episodes*100}%\")\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time \n",
    "\n",
    "state = environment.reset()\n",
    "done = False\n",
    "sequence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b6cf94-dffb-4509-b62b-f904d470cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "while not done:\n",
    "    # Choose the action with the highest value in the current state\n",
    "    if np.max(qtable[state]) > 0:\n",
    "      action = np.argmax(qtable[state])\n",
    "\n",
    "    # If there's no best action (only zeros), take a random one\n",
    "    else:\n",
    "      action = environment.action_space.sample()\n",
    "    \n",
    "    # Add the action to the sequence\n",
    "    sequence.append(action)\n",
    "\n",
    "    # Implement this action and move the agent in the desired direction\n",
    "    new_state, reward, done, truncated, info = environment.step(action)\n",
    "\n",
    "    # Update our current state ----------------\n",
    "    if(type(new_state)==list):\n",
    "        state =new_state\n",
    "    else:\n",
    "        state=[new_state]\n",
    "\n",
    "    # Update the render\n",
    "    clear_output(wait=True)\n",
    "    environment.render()\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"Sequence = {sequence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
