{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad186627-b6e8-4a35-8625-b0220e211f51",
   "metadata": {},
   "source": [
    "# Actor-Critic Method\n",
    "Actor: a policy function parameterized by theta: $\\pi_{\\theta}(s)$\\\n",
    "Critic: a value function parameterized by w: $\\hat{q}_{w}(s,a)$\\\n",
    "\n",
    "Step 1: Our Policy takes the state and outputs an action $A_{t}$\\\n",
    "Step 2a: The Critic takes that action as an input and using $S_{t}$ and $A_{t}$. It computes the value of that action at that state: the Q-value.\\\n",
    "Step 2b: The Actor updates its policy parameters using the Q value.\\ \n",
    "Step 3: The action $A_{t}$ performed in the environment outputs a new state $S_{t+1}$ and a reward $R_{t+1}$.\\\n",
    "Step 4: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0641288-fe1f-4d60-a8f6-a25e5faf96b9",
   "metadata": {},
   "source": [
    "## Soft Actor-Critic\n",
    " SAC is an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. The actor aims to maximize expected reward while also maximizing entropy. Entropy is a quantity which, roughly speaking, says how random a random variable is. The entropy H of x is computed from its distribution P according to\n",
    "\n",
    "$H(P)= E_{x \\sim P} [-logP(x)]$\n",
    "\n",
    "The actor gets a bonus reward at each time step proportional to the entropy of the policy at that timestep. In entropy-regularized reinforcement learning, the agent gets a bonus reward at each time step proportional to the entropy of the policy at that timestep. This changes the RL problem to:\n",
    "\n",
    "$\\pi^* = \\arg \\max_{\\pi} E_{\\tau \\sim \\pi}{ \\sum_{t=0}^{\\infty} \\gamma^t \\bigg( R(s_t, a_t, s_{t+1}) + \\alpha H\\left(\\pi(\\cdot|s_t)\\right) \\bigg)},$\n",
    "\n",
    "where $\\alpha > 0$ is the trade-off coefficient.\n",
    "We can define a different value functions in this setting. $V^{\\pi}$ is changed to include the entropy bonuses from every timestep:\n",
    "\n",
    "$V^{\\pi}(s) = E_{\\tau \\sim \\pi}{ \\left. \\sum_{t=0}^{\\infty} \\gamma^t \\bigg( R(s_t, a_t, s_{t+1}) + \\alpha H\\left(\\pi(\\cdot|s_t)\\right) \\bigg) \\right| s_0 = s}$\n",
    "\n",
    "$Q^{\\pi}$ is changed to include the entropy bonuses from every timestep except the first:\n",
    "\n",
    "$Q^{\\pi}(s,a) = E_{\\tau \\sim \\pi}{ \\left. \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t, s_{t+1}) + \\alpha \\sum_{t=1}^{\\infty} \\gamma^t H\\left(\\pi(\\cdot|s_t)\\right)\\right| s_0 = s, a_0 = a}$\n",
    "\n",
    "$V^{\\pi}$ and $Q^{\\pi}$ are connected by:\n",
    "\n",
    "$V^{\\pi}(s) = E_{a \\sim \\pi}[{Q^{\\pi}(s,a)} + \\alpha H\\left(\\pi(\\cdot|s)\\right)]$\n",
    "\n",
    "\n",
    "\n",
    "$Q^{\\pi}(s,a) = E_{s' \\sim P}({R(s,a,s') + \\gamma V^{\\pi}(s'))}.$\n",
    "\n",
    "SAC concurrently learns a policy $\\pi_{\\theta}$ and two Q-functions $Q_{\\phi_1}, Q_{\\phi_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c18ea-e4be-4d81-a01b-659e2711689c",
   "metadata": {},
   "source": [
    "### Learning Q\n",
    "\n",
    "Both Q-functions are learned with MSBE minimization, by regressing to a single shared target. The shared target is computed using target Q-networks, and the target Q-networks are obtained by polyak averaging the Q-network parameters. shared target makes use of the clipped double-Q trick.shared target makes use of the **clipped double-Q trick**(Clipped Double Q-learning is a variant on Double Q-learning that upper-bounds the less biased $Q_{\\theta_{1}}$ estimate by the biased estimate of $Q_{\\theta_{2}})$.\n",
    "The next-state actions used in the target come from the current policy instead of a target policy.  SAC trains a stochastic policy so there is no target policy smoothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189294c5-e34e-461b-8cf1-17a0c9e09d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
