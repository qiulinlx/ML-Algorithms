{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edba700b-eae6-4834-833f-018df98fe064",
   "metadata": {},
   "source": [
    "# Temporal Differences\n",
    "\n",
    "Temporal Difference Learning (Unsupervised technique in which the learning agent learns to predict the expected value of a variable occurring at the end of a sequence of states.) Both Temporal Difference algorithms (SARSA and Q-learning) are **model-free** which means that actions are associated with values and the agent acts off that. Agents will choose the action with the maximum value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab01c96-4082-409e-afc0-850ea0b016c8",
   "metadata": {},
   "source": [
    "## SARSA (aka. State-Action-Reward-State-Action)\n",
    "\n",
    "In SARSA, the agent uses the On-policy for learning where the agent learns from the current set of actions in the current state and the target policy or the action to be performed.\\\n",
    "Let us begin implementing the algorithm in the pre-made OpenAI gym environment, FrozenLake-v1. We start by importing the necessary packages and rendering the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84299e7-fab6-4129-8e6b-3434c28cd565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d384ffe-c053-4276-afec-e42b7c3fd5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode='human')\n",
    "environment.reset()\n",
    "environment.render()\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fca0d7-43e1-4f7c-99f7-f9ee96df2c19",
   "metadata": {},
   "source": [
    "Similarly to Q-learning, we will setup a Q-table and fill it with action-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f826362-44f7-4647-be39-183b374fffef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set up the Q-table\n",
    "Q = np.zeros((n_states, n_actions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c215f35-342c-4088-81ce-1c52bb7eeca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "alpha = 0.6  # learning rate\n",
    "gamma = 0.5  # discount factor\n",
    "epsilon = 0.5  # exploration rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13bf8e9-ef8d-4f30-8aa1-1c4b8f4da73d",
   "metadata": {},
   "source": [
    "We now begin the training portion of the algorithm. This is where the agent explores the environment and gathers data on the q-values of the various states and actions. This is almost identical to the training in Q-learning with the exception of the Bellman equation which is slightly modified. It becomes: \n",
    "\n",
    "$New  Q(s,a)= Q(s,a) + \\alpha [R(s,a) +\\gamma Q(s',a') +Q(s,a)]$\n",
    "\n",
    "The only difference is that instead of $Max Q(s', a')$ we just use Q(s', a'). Previous states and actions are not considered during the implementation of the next action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6382ee5f-15b8-4a79-85c2-560f1de4df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "for episode in range(100):\n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    state=np.asarray(state)\n",
    "    \n",
    "    # Choose the initial action using epsilon-greedy policy\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = env.action_space.sample()  # explore\n",
    "    else:\n",
    "        action = np.argmax(Q[state[0]])  # exploit\n",
    "    \n",
    "    # Loop over steps in the episode\n",
    "    while True:\n",
    "        # Take the chosen action and observe the next state and reward\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Choose the next action using epsilon-greedy policy\n",
    "        if np.random.rand() < epsilon:\n",
    "            next_action = env.action_space.sample()  # explore\n",
    "        else:\n",
    "            next_action = np.argmax(Q[next_state])  # exploit\n",
    "        \n",
    "        # Update the Q-value of the current state-action pair\n",
    "        Q[state[0], action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state[0], action])\n",
    "        \n",
    "        # Update the current state and action\n",
    "        state[0] = next_state\n",
    "        action = next_action\n",
    "        \n",
    "        # Check if the episode is over\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542ec641-7f20-483e-81f0-17017b8fdcd1",
   "metadata": {},
   "source": [
    "Now that we are done exploring (Training) we can test the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d049f-176a-42e5-bc18-14bc5da596df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "n_success = 0\n",
    "for episode in range(100):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(Q[state[0]])\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        #print(info)\n",
    "    if done:\n",
    "        n_success += reward\n",
    "print('Success rate:', n_success / 100)\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9533af1-c70d-49b7-b569-28b61ec853b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
