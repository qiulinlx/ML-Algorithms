{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0818751e-3310-4a13-bf0c-61800cec4ebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d387dce-190a-45de-a910-0483b39e5716",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basics of Reinforcement Learning\n",
    "Reinforcement learning is about taking suitable action to maximize reward in a particular situation. It is made up of three parts.\\\n",
    "**Agent** that interacts with the environment via actions.\\\n",
    "**Action space** is the set of all possible actions in an environment.Discrete spaces are finite whereas continuous states are infinite.\\\n",
    "**Environment** contains information about all possible actions that an agent can take, the states that agent can achieve following those actions, and the reward or penalty (negative reward) the environment gives to the agent, in return of the interaction.\\\n",
    "**States** are the information our agent gets from the environment. This must be a complete description of the state world, otherwise it is known as an **Observation** (partial description of the state)\\\n",
    "**Policy** a strategy which applies to the agent to decide the next action based on the current state/ observation\\\n",
    "**Reward** Immediate reward given when the agent performs a specific task\\\n",
    "**Value (V)** It is the expected long-term return with discount, as compared to the short-term reward.\\\n",
    "\n",
    "Discounting is the process of reducing the reward at each timestep to ensure that the agent reaches the optimal value/ policy in the least amount of steps.\n",
    "\n",
    "There are two types of tasks that an RL algorithm can solve. The **Episodic Tasks** have a start and end point. **Continuing tasks** require the agent learn how to choose the best actions and simultaneously interact with the environment. The agent will keep running until it is told to stop\n",
    "\n",
    "### RL Algorithms \n",
    "There are three approaches to finding solutions\n",
    "- Value-Based: The goal is to try to maximize a value function V(s) \n",
    "- Policy-Based: Come up with a policy such that the  action performed in every state helps you to gain maximum reward in the future.\n",
    "- Model-Based: Create a virtual model for each environment. The agent learns to perform in that specific environment. \n",
    "\n",
    "### Exploration vs Exploitation\n",
    "Greedy and $\\epsilon$ greedy\n",
    "\n",
    "### Challenges of RL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702b521-cb02-492a-9ed0-4afea1b4c146",
   "metadata": {},
   "source": [
    "## What is dynamic programming?\n",
    "\n",
    "Dynamic Programming is a collection of algorithms created to explore environments that are finite **Markov Decision Process**. They are used typically to solve planning problems. Policy evaluation is the task of determining the state-value function $v_\\pi$ for a given policy $\\pi$. There are two types of Dynamic Programming algorithms\n",
    "<ol>\n",
    "  <li>Policy Iteration</li>\n",
    "  <li>Value Iteration</li>\n",
    "\n",
    "</ol> \n",
    "Both of these approaches are **Model Based** algorithms agent trying to understand its environment and creating a model for it based on its interactions with this environment. Let us begin implementing the Policy iteration algorithm in the pre-made OpenAI gym environment, FrozenLake-v1. We start by importing the necessary packages and rendering the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c803739-0411-4ff4-adec-220b3463bfb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6693b712-96d4-45bb-b3e6-910d1ff1455e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode='human')\n",
    "# enwrap it to have additional information from it\n",
    "env = env.unwrapped\n",
    "env.reset()\n",
    "env.render()\n",
    "# spaces dimension\n",
    "nA = env.action_space.n\n",
    "nS = env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f74e686-386c-4bef-b651-e4a70b541dba",
   "metadata": {},
   "source": [
    "We start by choosing an arbitrary policy $\\pi$ .We then iteratively evaluate and improve the policy based on the value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df860695-f355-4cc7-9dbe-acf907ea9cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initializing value function and policy\n",
    "V = np.zeros(nS)\n",
    "policy = np.zeros(nS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eead1654-eefa-48a9-9ab8-f7974d34b604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some useful variable\n",
    "policy_stable = False\n",
    "it = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7879dd99-9dc6-4645-add9-0fd681e7d912",
   "metadata": {},
   "source": [
    "We can begin defining some functions necessary for the algorithm. The first function defines the value function (State-action pairs) of the various states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abbba492-8652-49c8-a873-3b86b22458c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_state_action(V, s, a, gamma=0.5):\n",
    "    return np.sum([p * (rew + gamma*V[next_s]) for p, next_s, rew, _ in env.P[s][a]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c026f699-4d18-42f2-b3f3-bd1c1ae18ad4",
   "metadata": {},
   "source": [
    "This next function allows us to evaluate our policy. Whether the policy is optimal is decided by this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be5c535-a54a-4731-b98d-544dd95ee1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(V, policy, eps=0.0001):\n",
    "    '''\n",
    "    Policy evaluation. Update the value function until it reach a steady state\n",
    "    '''\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # loop over all states\n",
    "        for s in range(nS):\n",
    "            old_v = V[s]\n",
    "            # update V[s] using the Bellman equation\n",
    "            V[s] = eval_state_action(V, s, policy[s])\n",
    "            delta = max(delta, np.abs(old_v - V[s]))\n",
    "\n",
    "        if delta < eps:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71947d-f36b-4450-933f-74e62604bf69",
   "metadata": {},
   "source": [
    "This is the function we use to update the policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e11a207-cbaa-4b30-8ac3-27b6c57d4535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def policy_improvement(V, policy):\n",
    "    '''\n",
    "    Policy improvement. Update the policy based on the value function\n",
    "    '''\n",
    "    policy_stable = True\n",
    "    for s in range(nS):\n",
    "        old_a = policy[s]\n",
    "        # update the policy with the action that bring to the highest state value\n",
    "        policy[s] = np.argmax([eval_state_action(V, s, a) for a in range(nA)])\n",
    "        if old_a != policy[s]: \n",
    "            policy_stable = False\n",
    "\n",
    "    return policy_stable\n",
    "\n",
    "    while not policy_stable:\n",
    "        policy_evaluation(V, policy)\n",
    "        policy_stable = policy_improvement(V, policy)\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7298067-7d46-46bb-b96b-660da8bae05d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Running the episode is equivalent to testing the agent. We want to know how well the agent will perform in the environment if it obeys the optimal policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea09ef1-bcc7-49e2-b6b8-59c0d4c43ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_episodes(env, policy, num_games=100):\n",
    "    '''\n",
    "    Run some games to test a policy\n",
    "    '''\n",
    "    tot_rew = 0\n",
    "    state = env.reset()\n",
    "    #print(type(state[0]))\n",
    "    #print(type(policy))\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        done = False\n",
    "        while not done:\n",
    "            # select the action accordingly to the policy\n",
    "#             break\n",
    "            next_state, reward, done,_,_ = env.step(int(policy[state[0]]))\n",
    "            state=np.asarray(state)    \n",
    "            state[0] = next_state\n",
    "            print(state)\n",
    "            tot_rew += reward \n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                state=np.asarray(state)\n",
    "\n",
    "            print('Won %i of %i games!'%(tot_rew, num_games))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9776ec61-dcff-4379-a6a3-5f244ea7483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Converged after %i policy iterations'%(it))\n",
    "run_episodes(env, policy)\n",
    "print(V.reshape((4,4)))\n",
    "print(policy.reshape((4,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d47cf754-6f2d-465d-92f6-3dfa971107af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70f03a-a7ad-4554-ba57-52650c31ba20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
