{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb89c67-aebc-4e37-b513-0e9f1c32e5d5",
   "metadata": {},
   "source": [
    "# Deep Q Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffe596e-e132-47e6-a5c5-c7184cf1a81c",
   "metadata": {},
   "source": [
    "Developed by Deepmind in 2015. The idea of storing all the values for each state action pair becomes increasingly unfeasible given larger environments and consequently more states and actions.\\\n",
    "Deep Q-Learning which uses a deep neural network to approximate the state-action values instead of computing Q exactly. The goal of DQN is to train a policy that tries to maximize the discounted, cumulative reward\\\n",
    "We will show how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole\n",
    "\n",
    "\n",
    "The aim will be to train a policy that tries to maximize the discounted, cumulative reward where $R_{t_{0}}$ is the return\\\n",
    "$R_{t0}=\\Sigma^{\\infty}_{t=t_{0}}\\gamma^{t-t_{0}}r_{t}$\\\n",
    "A lower $\\gamma$ makes rewards from the uncertain far future less important.\\\n",
    "The main idea behind Q-learning is that if we have a function $Q(s,a)$ that tells us what our return would be, if we were to take an action in a given state, then we could easily construct a policy that maximizes our rewards. Since we can't know everything about the world. We can approximate Q using a neural network. Sice every Q function for some policy obeys the Bellman equation.\n",
    "\n",
    "$Q^{\\pi}=r+ \\gamma Q^{\\pi}(s', \\pi(s'))$\n",
    "\n",
    "The difference between the two sides is known as the temporal difference error $\\delta$ \n",
    "\n",
    "$\\delta= Q^{\\pi}-(r+ \\gamma Q^{\\pi}(s', \\pi(s')))$\n",
    "\n",
    "We will aim to minimise $\\delta$ by using a loss function. In this case we use the Huber Loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664b24c3-810b-42a7-a365-1b88849c8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb3660-a419-4891-808d-a8e79d536ace",
   "metadata": {},
   "source": [
    "We will use the Pytorch framework for the \"Deep\" part of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ded698-7b88-4850-99f7-5cc4c0669ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e671b57a-eb4c-43b8-8f47-5742cc765482",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d54de-a57a-4cc1-914e-c6c94fdbdff6",
   "metadata": {},
   "source": [
    "### Experience Replay & Replay Memory\n",
    "\n",
    "Experience replay is a technique used in off-policy reinforcement learning. It improve sample efficiency and stability. The technique consists of storing a fixed number of the most recently collected\n",
    "transitions for training in a data structure known as Replay memory. (aka storing the agents experiences at each time step $t$)\\\n",
    "By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure since this data is more like independent and identically distributed random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38436c4c-ba0c-4468-bc88-935edc8f51bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c429b-4dda-4cab-98d3-b83b7500a924",
   "metadata": {},
   "source": [
    "We have a neural network with a three layers of 128 peceptron. The activation function of each layer is relu. The network is trying to predict the expected return of taking each action given the current input. The outputs represent the Q-values for the possble actions in the given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbc59fc-3b04-49e9-9eef-472d51d284b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d0c3b-d05f-494f-8d06-9af0c3c4676e",
   "metadata": {},
   "source": [
    " BATCH_SIZE is the number of transitions sampled from the replay buffer\\\n",
    " GAMMA is the discount factor as mentioned in the previous section\\\n",
    " EPS_START is the starting value of epsilon\\\n",
    " EPS_END is the final value of epsilon\\\n",
    " EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\\\n",
    " TAU is the update rate of the target network\\\n",
    " LR is the learning rate of the AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71aef36-bf74-4f70-8f06-8be5a5da69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.95\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c515e3-877d-4ac3-94ed-64b6f6b5b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88aebd9-ef89-42b9-a2d2-0c08f15431ad",
   "metadata": {},
   "source": [
    "We have two neural nets that have the exact same structure.\n",
    "1. The Policy Network. The state is given as the input and the Q-value of all possible actions is generated as the output.\n",
    "2. The Target Network. This target network has the same architecture as the function approximator but with frozen parameters. For every C iterations (a hyperparameter), the parameters from the prediction network are copied to the target network. This leads to more stable training because it keeps the target function fixed (for a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ecb3e-3a37-4db8-ad58-cd0ec0305651",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb3fe12-ad41-4762-9a53-49d1bb155656",
   "metadata": {},
   "source": [
    "select_action - will select an action according to an epsilon greedy policy. Simply put, we’ll either use our model to choose the action, and sometimes we’ll just sample one uniformly. The probability of choosing a random action will start at EPS_START and will decay exponentially towards EPS_END. EPS_DECAY controls the rate of the decay. This is known as Epsilon decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb3448-fadc-4c2d-9097-7b0909d2ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97983644-e5ae-4c90-9d58-cc81f87795cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273585f-260a-43bc-b8d9-75462a95e5ae",
   "metadata": {},
   "source": [
    "'optimize_model' function that performs a single step of the optimization. It first samples a batch from the memory and computes it as $Q(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614a7d5-8b81-4ab9-9efb-201b32ca76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken. \n",
    "    #These are the actions which would've been taken for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based  on the \"older\" target_net;\n",
    "    #selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeac45c-3392-4242-80f9-62b7311e2770",
   "metadata": {},
   "source": [
    "Below, you can find the main training loop.\n",
    "At the beginning we reset the environment and obtain the initial state Tensor. Then, we sample an action, execute it, observe the next state and the reward (always 1), and optimize our model once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab70d7-74d4-48c5-bc54-81afcf6e0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 100\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a0798-7e9d-4d4b-a9d9-27216e7e33b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
